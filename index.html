<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.8/dist/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.8/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.8/dist/index.js"></script><script>(e=>{window.WebFontConfig={custom:{families:["KaTeX_AMS","KaTeX_Caligraphic:n4,n7","KaTeX_Fraktur:n4,n7","KaTeX_Main:n4,n7,i4,i7","KaTeX_Math:i4,i7","KaTeX_Script","KaTeX_SansSerif:n4,n7,i4","KaTeX_Size1","KaTeX_Size2","KaTeX_Size3","KaTeX_Size4","KaTeX_Typewriter"]},active:()=>{e().refreshHook.call()}}})(()=>window.markmap)</script><script src="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.js" defer></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"Natural Language Processing (NLP) Mind Map with Explanations","children":[{"content":"<strong>Definition</strong>","children":[{"content":"Interdisciplinary field enabling computers to understand, interpret, and generate human language.","children":[],"payload":{"tag":"li","lines":"9,11"}}],"payload":{"tag":"h2","lines":"8,9"}},{"content":"<strong>Core Areas</strong>","children":[{"content":"<strong>Natural Language Understanding (NLU):</strong> Enables computers to comprehend and extract meaning from text and speech, focusing on tasks like intent detection and entity recognition.","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"<strong>Natural Language Generation (NLG):</strong> Enables computers to generate human-like text, such as writing articles, creating responses, or producing summaries.","children":[],"payload":{"tag":"li","lines":"13,15"}}],"payload":{"tag":"h2","lines":"11,12"}},{"content":"<strong>Applications</strong>","children":[{"content":"<strong>Sentiment Analysis:</strong> Detects emotions (positive, negative, neutral) expressed in text.","children":[],"payload":{"tag":"li","lines":"16,17"}},{"content":"<strong>Toxicity Classification:</strong> Identifies harmful or abusive language in content.","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"<strong>Machine Translation:</strong> Automatically converts text from one language to another.","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"<strong>Named Entity Recognition:</strong> Identifies and classifies entities like names, dates, and locations.","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"<strong>Spam Detection:</strong> Filters out unwanted or irrelevant messages, such as email spam.","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"<strong>Grammatical Error Correction:</strong> Detects and corrects grammatical mistakes in text.","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"<strong>Topic Modeling:</strong> Discovers hidden themes or topics in a collection of documents.","children":[],"payload":{"tag":"li","lines":"22,23"}},{"content":"<strong>Text Generation:</strong> Produces coherent and contextually relevant text.","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"<strong>Autocomplete:</strong> Predicts and completes user input, often in search engines or text editors.","children":[],"payload":{"tag":"li","lines":"24,25"}},{"content":"<strong>Chatbots:</strong> Simulates human conversation for customer support or personal assistance.","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"<strong>Information Retrieval:</strong> Finds relevant information in large datasets or the web.","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"<strong>Summarization:</strong> Condenses text into a shorter version while retaining key points.","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"<strong>Question Answering:</strong> Provides accurate answers to user queries, often from a knowledge base.","children":[],"payload":{"tag":"li","lines":"28,30"}}],"payload":{"tag":"h2","lines":"15,16"}},{"content":"<strong>Key Terms</strong>","children":[{"content":"<strong>Document:</strong> A single unit of text, such as a paragraph or an article.","children":[{"content":"<strong>Example</strong>:","children":[{"content":"A single news article from a newspaper.","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"A tweet: \"Just watched an amazing movie!\"","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"An email: \"Dear John, I hope this email finds you well…\"","children":[],"payload":{"tag":"li","lines":"35,36"}}],"payload":{"tag":"li","lines":"32,36"}}],"payload":{"tag":"li","lines":"31,36"}},{"content":"<strong>Corpus:</strong> A collection of documents used for training or analyzing NLP models.","children":[{"content":"<strong>Example</strong>:","children":[{"content":"A collection of all articles from a specific newspaper over a year.","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"A dataset of customer reviews from an e-commerce website.","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"The Gutenberg Corpus: A collection of literary texts from Project Gutenberg.","children":[],"payload":{"tag":"li","lines":"40,41"}}],"payload":{"tag":"li","lines":"37,41"}}],"payload":{"tag":"li","lines":"36,41"}},{"content":"<strong>Feature:</strong> A measurable property of text, such as word count or specific keywords.","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"h2","lines":"30,31"}},{"content":"<strong>How NLP Works</strong>","children":[{"content":"1. <strong>Data Preprocessing</strong>","children":[{"content":"<strong>Tokenization:</strong> Splitting text into smaller units (tokens), like words or sentences.","children":[{"content":"<strong>Types:</strong>","children":[{"content":"<strong>Word Tokenization:</strong> Splitting text into individual words.","children":[{"content":"<strong>Example</strong>:","children":[{"content":"<strong>Input:</strong> \"I study Machine Learning on Youtube.\"","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"<strong>Output:</strong> [\"I\" \"study\", \"Machine\", \"Learning\", \"on\", \"Youtube\", \".\"]","children":[],"payload":{"tag":"li","lines":"50,51"}}],"payload":{"tag":"li","lines":"48,51"}}],"payload":{"tag":"li","lines":"47,51"}},{"content":"<strong>Sentence Tokenization:</strong> Splitting text into individual sentences.","children":[{"content":"<strong>Example</strong>:","children":[{"content":"<strong>Input:</strong> \"I study Machine Learning on Youtube. Currently, I'm studying NLP.\"","children":[],"payload":{"tag":"li","lines":"53,54"}},{"content":"<strong>Output:</strong> [\"I study Machine Learning on Youtube.\", \"Currently, I'm studying NLP.\"]","children":[],"payload":{"tag":"li","lines":"54,55"}}],"payload":{"tag":"li","lines":"52,55"}}],"payload":{"tag":"li","lines":"51,55"}}],"payload":{"tag":"li","lines":"46,55"}},{"content":"<strong>Importance:</strong> Tokenization is the first step in many NLP pipelines and affects the subsequent processing stages.","children":[],"payload":{"tag":"li","lines":"55,56"}}],"payload":{"tag":"li","lines":"45,56"}},{"content":"<strong>Stemming:</strong> Reducing words to their base form.","children":[{"content":"<strong>Example:</strong>","children":[{"content":"\"Running\" -&gt; \"Run\"","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"\"Runner\" -&gt; \"Run\"","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"\"Studies\" -&gt; \"Studi\"","children":[],"payload":{"tag":"li","lines":"60,61"}}],"payload":{"tag":"li","lines":"57,61"}}],"payload":{"tag":"li","lines":"56,61"}},{"content":"<strong>Lemmatization:</strong> Converting words to their dictionary form.","children":[{"content":"<strong>Example:</strong>","children":[{"content":"\"Running\" -&gt; \"Run\"","children":[],"payload":{"tag":"li","lines":"63,64"}},{"content":"\"Plays\" -&gt; \"Play\"","children":[],"payload":{"tag":"li","lines":"64,65"}},{"content":"\"Played\" -&gt; \"Play\"","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"\"Communication\" -&gt; \"Communication\"","children":[],"payload":{"tag":"li","lines":"66,67"}},{"content":"\"I'm\", \"are\", \"is\" -&gt; \"be\"","children":[],"payload":{"tag":"li","lines":"67,68"}}],"payload":{"tag":"li","lines":"62,68"}}],"payload":{"tag":"li","lines":"61,68"}},{"content":"<strong>Normalization:</strong> Converting text into a consistent format. to ensure consistency in how text data is processed.","children":[{"content":"<strong>Lowercasing</strong>: \"Apple\" -&gt; \"apple\"","children":[],"payload":{"tag":"li","lines":"69,70"}},{"content":"<strong>Removing Punctuation</strong>: \"Hello, world!\" -&gt; \"Hello world\"","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"<strong>Removing Stop Words</strong>: \"This is a pen\" -&gt; \"pen\"","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"<strong>Example</strong>:","children":[{"content":"<strong>Input</strong>: \"Running is better than walking! Apples and oranges are different.\"","children":[],"payload":{"tag":"li","lines":"73,74"}},{"content":"<strong>Output</strong>:","children":[{"content":"<strong>Lowercased text:</strong> running is better than walking! apples and oranges are different.","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"<strong>Text without punctuation:</strong> running is better than walking apples and oranges are different","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"<strong>Tokenized words:</strong> ['running', 'is', 'better', 'than', 'walking', 'apples', 'and', 'oranges', 'are', 'different']","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"<strong>Text without stopwords:</strong> ['running', 'better', 'walking', 'apples', 'oranges', 'different']","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"<strong>Stemmed words:</strong> ['run', 'better', 'walk', 'appl', 'orang', 'differ']","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"<strong>Lemmatized words:</strong> ['running', 'better', 'walking', 'apple', 'orange', 'different']","children":[],"payload":{"tag":"li","lines":"80,81"}}],"payload":{"tag":"li","lines":"74,81"}}],"payload":{"tag":"li","lines":"72,81"}}],"payload":{"tag":"li","lines":"68,81"}},{"content":"<strong>POS Tagging:</strong> Assigning grammatical categories to words (e.g., noun, verb).","children":[{"content":"<strong>Importance</strong>: Helps in understanding the grammatical structure and meaning of sentences.","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"<strong>Examples:</strong>","children":[{"content":"<strong>Input:</strong> \"GeeksforGeeks is a Computer Science platform.\"","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"<strong>Output:</strong> [('GeeksforGeeks', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('Computer', 'NNP'), ('Science', 'NNP'), ('platform', 'NN'), ('.', '.')]","children":[],"payload":{"tag":"li","lines":"85,87"}}],"payload":{"tag":"li","lines":"83,87"}}],"payload":{"tag":"li","lines":"81,87"}}],"payload":{"tag":"h3","lines":"44,45"}},{"content":"2. <strong>Feature Extraction</strong>","children":[{"content":"<strong>Bag-of-Words (BoW):</strong> Represents text as a set of words with their frequencies, ignoring context.","children":[{"content":"<strong>Importance:</strong> Simplifies text processing and is used in various NLP applications such as text classification and information retrieval.","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"<strong>Example:</strong>","children":[{"content":"<strong>Input:</strong> documents = [\"Barack Obama was the 44th President of the United States\", \"The President lives in the White House\", \"The United States has a strong economy\"]","children":[],"payload":{"tag":"li","lines":"91,92"}},{"content":"<strong>Output:</strong>","children":[],"payload":{"tag":"li","lines":"92,93"}},{"content":"<strong>Feature Names (Words):</strong> ['44th' 'barack' 'economy' 'has' 'house' 'in' 'lives' 'obama' 'of' 'president' 'states' 'strong' 'the' 'united' 'was' 'white']","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"<strong>Bag of Words Representation:</strong> [[1 1 0 0 0 0 0 1 1 1 1 0 2 1 1 0], [0 0 0 0 1 1 1 0 0 1 0 0 2 0 0 1], [0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0]]","children":[],"payload":{"tag":"li","lines":"94,95"}}],"payload":{"tag":"li","lines":"90,95"}},{"content":"![[Pasted image 20250117180000.png]]","children":[],"payload":{"tag":"li","lines":"95,96"}}],"payload":{"tag":"li","lines":"88,96"}},{"content":"<strong>One-Hot Encoding:</strong> One-hot encoding represents individual words or characters as binary vectors, where each vector corresponds to a unique word, and only one element is set to 1 (the \"hot\" element), while all others are set to 0.","children":[{"content":"<strong>Importance:</strong>","children":[{"content":"Simplifies categorical data for machine learning models.","children":[],"payload":{"tag":"li","lines":"98,99"}},{"content":"Avoids ordinal relationships between categories.","children":[],"payload":{"tag":"li","lines":"99,100"}}],"payload":{"tag":"li","lines":"97,100"}},{"content":"<strong>Example:</strong>","children":[{"content":"<strong>Vocabulary:</strong> [\"I\", \"love\", \"NLP\", \"enjoy\", \"learning\"]","children":[],"payload":{"tag":"li","lines":"101,102"}},{"content":"<strong>Sentence Representation:</strong>","children":[{"content":"\"I love NLP.\" → [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0]]","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"\"I enjoy learning NLP.\" → [[1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0]]","children":[],"payload":{"tag":"li","lines":"104,105"}}],"payload":{"tag":"li","lines":"102,105"}}],"payload":{"tag":"li","lines":"100,105"}}],"payload":{"tag":"li","lines":"96,105"}},{"content":"<strong>TF-IDF:</strong> Weighs words based on their frequency in a document relative to the corpus.","children":[{"content":"<strong>Term Frequency (TF)</strong>: Measures how frequently a term appears in a document. It is often normalized by the total number of terms in the document to prevent bias toward longer documents.","children":[{"content":"\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>T</mi><mi>F</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo separator=\"true\">,</mo><mi>d</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mtext>Number&nbsp;of&nbsp;times&nbsp;term&nbsp;t&nbsp;appears&nbsp;in&nbsp;document&nbsp;d</mtext><mtext>Total&nbsp;number&nbsp;of&nbsp;terms&nbsp;in&nbsp;document&nbsp;d</mtext></mfrac></mrow><annotation encoding=\"application/x-tex\">TF(t,d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TF</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0574em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">Total&nbsp;number&nbsp;of&nbsp;terms&nbsp;in&nbsp;document&nbsp;d</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">Number&nbsp;of&nbsp;times&nbsp;term&nbsp;t&nbsp;appears&nbsp;in&nbsp;document&nbsp;d</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>","children":[],"payload":{"tag":"li","lines":"107,108"}}],"payload":{"tag":"li","lines":"106,108"}},{"content":"<strong>Inverse Document Frequency (IDF)</strong>: Measures how important a term is in the entire corpus. It decreases the weight of terms that appear in many documents and increases the weight of terms that appear in fewer documents.","children":[{"content":"\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo separator=\"true\">,</mo><mi>D</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mfrac><mtext>Total&nbsp;number&nbsp;of&nbsp;documents&nbsp;in&nbsp;corpus&nbsp;D</mtext><mtext>Number&nbsp;of&nbsp;documents&nbsp;where&nbsp;term&nbsp;t&nbsp;appears&nbsp;+&nbsp;1</mtext></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">IDF(t, D) = log(\\frac{\\text{Total number of documents in corpus D}}{\\text{Number of documents where term t appears + 1}})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.2519em;vertical-align:-0.8804em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">Number&nbsp;of&nbsp;documents&nbsp;where&nbsp;term&nbsp;t&nbsp;appears&nbsp;+&nbsp;1</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">Total&nbsp;number&nbsp;of&nbsp;documents&nbsp;in&nbsp;corpus&nbsp;D</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8804em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span></span>","children":[],"payload":{"tag":"li","lines":"109,110"}}],"payload":{"tag":"li","lines":"108,110"}},{"content":"<strong>Combining TF and IDF: TF-IDF</strong>: The TF-IDF score is calculated by multiplying the TF value with the IDF value for a term t in a document d:","children":[{"content":"\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo separator=\"true\">,</mo><mi>d</mi><mo separator=\"true\">,</mo><mi>D</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>T</mi><mi>F</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo separator=\"true\">,</mo><mi>d</mi><mo stretchy=\"false\">)</mo><mo>×</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo separator=\"true\">,</mo><mi>D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">TF-IDF(t, d, D) = TF(t, d) \\times IDF(t, D)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TF</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">TF</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span></span></span></span></span>","children":[],"payload":{"tag":"li","lines":"111,112"}}],"payload":{"tag":"li","lines":"110,112"}},{"content":"<strong>Importance</strong>: Helps in identifying important words in documents and is commonly used in information retrieval and text mining.","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"<strong>Example:</strong> [[TF-IDF Example]]","children":[],"payload":{"tag":"li","lines":"113,114"}},{"content":"<strong>Example:</strong> <a href=\"https://thoracic-provelone-25a.notion.site/TF-IDF-Example-17f0bd5f81d080f5bceac710bf1199cc\">Web</a>","children":[],"payload":{"tag":"li","lines":"114,115"}},{"content":"![[Pasted image 20250117185805.png]]","children":[],"payload":{"tag":"li","lines":"115,116"}}],"payload":{"tag":"li","lines":"105,116"}},{"content":"<strong>N-grams:</strong> Considers sequences of n words/characters to capture phrases.","children":[{"content":"<strong>Types:</strong>","children":[{"content":"<strong>Unigram</strong>: Single word.","children":[],"payload":{"tag":"li","lines":"118,119"}},{"content":"<strong>Bigram</strong>: Pair of words.","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"<strong>Trigram</strong>: Sequence of three words.","children":[],"payload":{"tag":"li","lines":"120,121"}}],"payload":{"tag":"li","lines":"117,121"}},{"content":"<strong>Importance</strong>: Captures context and word dependencies in text.","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"<strong>Example:</strong>","children":[{"content":"<strong>Input:</strong> \"The quick brown fox jumps over the lazy dog\"","children":[],"payload":{"tag":"li","lines":"123,124"}},{"content":"<strong>Output:</strong>","children":[{"content":"<strong>Unigrams:</strong> [('The',), ('quick',), ('brown',), ('fox',), ('jumps',), ('over',), ('the',), ('lazy',), ('dog',)]","children":[],"payload":{"tag":"li","lines":"125,126"}},{"content":"<strong>Bigrams:</strong> [('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]","children":[],"payload":{"tag":"li","lines":"126,127"}},{"content":"<strong>Trigrams:</strong> [('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog')]","children":[],"payload":{"tag":"li","lines":"127,128"}}],"payload":{"tag":"li","lines":"124,128"}}],"payload":{"tag":"li","lines":"122,128"}}],"payload":{"tag":"li","lines":"116,128"}},{"content":"<strong>Word Embeddings:</strong> Represents words as dense vectors capturing semantic meaning (e.g., Word2Vec).","children":[{"content":"<strong>Word2Vec:</strong>","children":[{"content":"Word2Vec is a neural network-based technique that creates word embeddings. It uses two primary architectures:","children":[{"content":"<strong>Skip-Gram:</strong> Predicts the context (neighboring words) given a target word.","children":[],"payload":{"tag":"li","lines":"131,132"}},{"content":"<strong>CBOW (Continuous Bag of Words):</strong> Predicts a target word based on its context (surrounding words).","children":[],"payload":{"tag":"li","lines":"132,133"}}],"payload":{"tag":"li","lines":"130,133"}},{"content":"<strong>Key Features:</strong>","children":[{"content":"Captures semantic relationships (e.g., \"king - man + woman ≈ queen\").","children":[],"payload":{"tag":"li","lines":"134,135"}},{"content":"Efficient for large datasets.","children":[],"payload":{"tag":"li","lines":"135,136"}}],"payload":{"tag":"li","lines":"133,136"}},{"content":"<strong>Limitation:</strong> Doesn't handle out-of-vocabulary (OOV) words well and treats words as atomic units, ignoring subword information.","children":[],"payload":{"tag":"li","lines":"136,137"}}],"payload":{"tag":"li","lines":"129,137"}},{"content":"<strong>Skip-Gram:</strong> A predictive model that trains a neural network to predict the context words around a given word in a sentence.","children":[{"content":"<strong>Input:</strong> Target word.","children":[],"payload":{"tag":"li","lines":"138,139"}},{"content":"<strong>Output:</strong> Probability distribution over all words in the vocabulary for likely context words.","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"<strong>Strengths:</strong>","children":[{"content":"Performs well on large corpora.","children":[],"payload":{"tag":"li","lines":"141,142"}},{"content":"Captures subtle semantic relationships effectively.","children":[],"payload":{"tag":"li","lines":"142,143"}}],"payload":{"tag":"li","lines":"140,143"}}],"payload":{"tag":"li","lines":"137,143"}},{"content":"<strong>FastText:</strong> An extension of Word2Vec developed by Facebook. It improves upon Word2Vec by representing words as a collection of character n-grams.","children":[{"content":"<strong>Advantages:</strong>","children":[{"content":"Handles rare and out-of-vocabulary words by generating embeddings based on subwords (e.g., \"jumping\" is represented as \"jum\", \"ump\", \"mpi\", \"pin\", etc.).","children":[],"payload":{"tag":"li","lines":"145,146"}},{"content":"Captures morphological variations better (e.g., plurals, conjugations).","children":[],"payload":{"tag":"li","lines":"146,147"}}],"payload":{"tag":"li","lines":"144,147"}},{"content":"<strong>Use Case:</strong> Effective for languages with rich morphology or large vocabularies.","children":[],"payload":{"tag":"li","lines":"147,148"}}],"payload":{"tag":"li","lines":"143,148"}},{"content":"<strong>Seq2Seq (Sequence-to-Sequence Models):</strong> A broader category of models designed for tasks involving input-output sequences of varying lengths (e.g., machine translation, text summarization).","children":[{"content":"<strong>Mechanism:</strong>","children":[{"content":"Uses an encoder-decoder architecture, often with RNNs, LSTMs, or Transformers.","children":[],"payload":{"tag":"li","lines":"150,151"}},{"content":"The encoder compresses the input sequence into a context vector.","children":[],"payload":{"tag":"li","lines":"151,152"}},{"content":"The decoder generates the output sequence based on this context.","children":[],"payload":{"tag":"li","lines":"152,153"}}],"payload":{"tag":"li","lines":"149,153"}},{"content":"<strong>Role in Embeddings:</strong> While not a word embedding method by itself, Seq2Seq models leverage embeddings (e.g., Word2Vec, FastText) as input representations.","children":[],"payload":{"tag":"li","lines":"153,154"}}],"payload":{"tag":"li","lines":"148,154"}}],"payload":{"tag":"li","lines":"128,154"}},{"content":"<strong>Contextual Word Embeddings:</strong>","children":[{"content":"Uses models like BERT to capture word meanings based on context.","children":[],"payload":{"tag":"li","lines":"155,156"}},{"content":"Dynamically generate word representations based on the sentence or context they appear in. This makes them highly effective for tasks where word meaning changes based on context.","children":[],"payload":{"tag":"li","lines":"156,157"}},{"content":"<strong>ELMo (Embeddings from Language Models):</strong> ELMo generates word embeddings by considering the context of a word in the entire sentence using bidirectional LSTM models. It produces different embeddings for the same word based on context.","children":[{"content":"<strong>Example:</strong>","children":[{"content":"<strong>\"Bank\" in \"The river bank was beautiful\"</strong> → embedding relates to nature.","children":[],"payload":{"tag":"li","lines":"159,160"}},{"content":"<strong>\"Bank\" in \"I need to deposit money at the bank\"</strong> → embedding relates to finance.","children":[],"payload":{"tag":"li","lines":"160,161"}}],"payload":{"tag":"li","lines":"158,161"}},{"content":"<strong>Advantages:</strong>","children":[{"content":"Handles polysemy (words with multiple meanings).","children":[],"payload":{"tag":"li","lines":"162,163"}},{"content":"Suitable for downstream NLP tasks like named entity recognition (NER) or sentiment analysis.","children":[],"payload":{"tag":"li","lines":"163,164"}}],"payload":{"tag":"li","lines":"161,164"}}],"payload":{"tag":"li","lines":"157,164"}},{"content":"<strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> BERT uses a transformer architecture and a bidirectional approach to understand context from both the left and right of a target word. Pre-training involves tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).","children":[{"content":"<strong>Example:</strong>","children":[{"content":"<strong>Sentence 1:</strong> \"I went to the bank to fish by the river.\"","children":[],"payload":{"tag":"li","lines":"166,167"}},{"content":"<strong>Sentence 2:</strong> \"I went to the bank to deposit my paycheck.\"","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"BERT generates different embeddings for \"bank\" in each sentence.","children":[],"payload":{"tag":"li","lines":"168,169"}}],"payload":{"tag":"li","lines":"165,169"}},{"content":"<strong>Advantages:</strong>","children":[{"content":"Highly contextualized and accurate.","children":[],"payload":{"tag":"li","lines":"170,171"}},{"content":"Performs well on tasks like question answering and machine translation.","children":[],"payload":{"tag":"li","lines":"171,172"}}],"payload":{"tag":"li","lines":"169,172"}}],"payload":{"tag":"li","lines":"164,172"}},{"content":"<strong>GPT (Generative Pre-trained Transformer):</strong> GPT is a unidirectional transformer, focusing on understanding the preceding context. While its embeddings are contextualized, they are less bidirectional than BERT.","children":[{"content":"<strong>Example:</strong>","children":[{"content":"Input: \"The cat sat on the...\"","children":[],"payload":{"tag":"li","lines":"174,175"}},{"content":"Embedding for \"cat\" considers previous context to predict and contextualize what follows (e.g., \"mat\" or \"floor\").","children":[],"payload":{"tag":"li","lines":"175,176"}}],"payload":{"tag":"li","lines":"173,176"}},{"content":"<strong>Advantages:</strong>","children":[{"content":"Excels in generative tasks like text generation and summarization.","children":[],"payload":{"tag":"li","lines":"177,178"}}],"payload":{"tag":"li","lines":"176,178"}}],"payload":{"tag":"li","lines":"172,178"}},{"content":"<strong>RoBERTa (Robustly Optimized BERT Pre-training Approach):</strong> RoBERTa improves on BERT by using larger datasets and longer training times, without the Next Sentence Prediction (NSP) task.","children":[{"content":"<strong>Example:</strong> Like BERT, it adjusts embeddings dynamically based on the sentence but is more robust for nuanced contexts.","children":[],"payload":{"tag":"li","lines":"179,180"}},{"content":"<strong>Advantages:</strong>","children":[{"content":"Better performance than BERT on many benchmarks.","children":[],"payload":{"tag":"li","lines":"181,182"}},{"content":"Optimized for broader generalization.","children":[],"payload":{"tag":"li","lines":"182,183"}}],"payload":{"tag":"li","lines":"180,183"}}],"payload":{"tag":"li","lines":"178,183"}},{"content":"<strong>Transformer-XL:</strong> Adds memory to the transformer architecture, capturing long-term dependencies across sentences or documents.","children":[{"content":"<strong>Example:</strong>","children":[{"content":"Input: \"The document discusses the importance of contextual embeddings. On the next page, the methods are explained.\"","children":[],"payload":{"tag":"li","lines":"185,186"}},{"content":"Transformer-XL retains the context of \"contextual embeddings\" to relate it to \"methods\".","children":[],"payload":{"tag":"li","lines":"186,187"}}],"payload":{"tag":"li","lines":"184,187"}},{"content":"<strong>Advantages:</strong> Handles long sequences effectively.","children":[],"payload":{"tag":"li","lines":"187,188"}}],"payload":{"tag":"li","lines":"183,188"}},{"content":"<strong>Advantages of Contextual Word Embeddings:</strong>","children":[{"content":"<strong>Handles Polysemy:</strong> Dynamically generates word embeddings specific to their meaning in a given context.","children":[],"payload":{"tag":"li","lines":"189,190"}},{"content":"<strong>Improved NLP Performance:</strong> Excels in tasks like sentiment analysis, machine translation, question answering, and text generation.","children":[],"payload":{"tag":"li","lines":"190,191"}},{"content":"<strong>Better Generalization:</strong> Models like BERT or RoBERTa are pre-trained on diverse datasets, making them suitable for many applications without much task-specific fine-tuning.","children":[],"payload":{"tag":"li","lines":"191,192"}},{"content":"<strong>Long-Range Dependency:</strong> Models like Transformer-XL capture dependencies across long sentences or paragraphs, improving coherence in text generation or summarization.","children":[],"payload":{"tag":"li","lines":"192,193"}}],"payload":{"tag":"li","lines":"188,193"}},{"content":"<strong>Applications:</strong>","children":[{"content":"<strong>Sentiment Analysis:</strong> Identifying nuanced emotions from text.","children":[],"payload":{"tag":"li","lines":"194,195"}},{"content":"<strong>Question Answering:</strong> Understanding and responding to queries.","children":[],"payload":{"tag":"li","lines":"195,196"}},{"content":"<strong>Text Summarization:</strong> Generating meaningful summaries for long documents.","children":[],"payload":{"tag":"li","lines":"196,197"}},{"content":"<strong>Machine Translation:</strong> Translating contextually accurate sentences between languages.","children":[],"payload":{"tag":"li","lines":"197,198"}},{"content":"<strong>NER (Named Entity Recognition):</strong> Recognizing entities like names, places, or organizations dynamically.","children":[],"payload":{"tag":"li","lines":"198,200"}}],"payload":{"tag":"li","lines":"193,200"}}],"payload":{"tag":"li","lines":"154,200"}}],"payload":{"tag":"h3","lines":"87,88"}},{"content":"3. <strong>Modeling</strong>","children":[{"content":"<strong>Traditional ML Techniques:</strong> Includes classifiers like Logistic Regression and Naive Bayes for text classification.","children":[],"payload":{"tag":"li","lines":"201,202"}},{"content":"<strong>Deep Learning Techniques:</strong> Uses advanced neural networks like CNNs, RNNs, and Transformers for sophisticated tasks.","children":[],"payload":{"tag":"li","lines":"202,204"}}],"payload":{"tag":"h3","lines":"200,201"}}],"payload":{"tag":"h2","lines":"43,44"}},{"content":"<strong>Model Comparison</strong>","children":[{"content":"<strong>TF-IDF vs. BoW:</strong> TF-IDF adds importance weighting, while BoW counts word occurrences.","children":[],"payload":{"tag":"li","lines":"205,206"}},{"content":"<strong>Word2Vec vs. TF-IDF:</strong> Word2Vec captures semantic relationships, unlike TF-IDF.","children":[],"payload":{"tag":"li","lines":"206,207"}},{"content":"<strong>RNNs vs. Word2Vec:</strong> RNNs process sequences over time; Word2Vec handles static word representations.","children":[],"payload":{"tag":"li","lines":"207,208"}},{"content":"<strong>LSTMs/GRUs vs. RNNs:</strong> Overcome RNN limitations like vanishing gradients for long sequences.","children":[],"payload":{"tag":"li","lines":"208,209"}},{"content":"<strong>Transformers vs. RNNs:</strong> Handle long dependencies efficiently and enable parallel processing.","children":[],"payload":{"tag":"li","lines":"209,211"}}],"payload":{"tag":"h2","lines":"204,205"}},{"content":"<strong>Test your Knowledge!</strong>","children":[{"content":"<strong>What are the two main areas of NLP, and how do they differ?</strong>","children":[{"content":"Natural Language Understanding (NLU) focuses on interpreting the meaning behind text, while Natural Language Generation (NLG) deals with producing human-like text.","children":[],"payload":{"tag":"li","lines":"213,214"}}],"payload":{"tag":"li","lines":"212,214"}},{"content":"<strong>Explain the difference between stemming and lemmatization. Why might one be preferred over the other in certain NLP tasks?</strong>","children":[{"content":"Stemming reduces words to their root form by stripping affixes, which can result in non-words. Lemmatization, on the other hand, reduces words to their base or dictionary form (lemma) and generally produces real words. Lemmatization is often preferred when accuracy is critical, while stemming is faster and less computationally expensive.","children":[],"payload":{"tag":"li","lines":"215,216"}}],"payload":{"tag":"li","lines":"214,216"}},{"content":"<strong>What is the purpose of tokenization in NLP, and what are the different types?</strong>","children":[{"content":"Tokenization breaks down text into smaller units called tokens, which can be words, subwords, or characters. Types include word tokenization, sentence tokenization, and subword tokenization.","children":[],"payload":{"tag":"li","lines":"217,218"}}],"payload":{"tag":"li","lines":"216,218"}},{"content":"<strong>How does Term Frequency-Inverse Document Frequency (TF-IDF) differ from the Bag of Words (BoW) model, and what problem does it solve?</strong>","children":[{"content":"TF-IDF improves upon BoW by not only counting word occurrences but also weighting words based on their importance in the document relative to the entire corpus. It addresses the problem of common words being overrepresented by reducing their importance.","children":[],"payload":{"tag":"li","lines":"219,220"}}],"payload":{"tag":"li","lines":"218,220"}},{"content":"<strong>What are word embeddings, and how do they improve upon traditional models like BoW and TF-IDF?</strong>","children":[{"content":"Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. Unlike BoW and TF-IDF, which treat words as discrete entities, word embeddings place semantically similar words close together in a continuous vector space, preserving context.","children":[],"payload":{"tag":"li","lines":"221,222"}}],"payload":{"tag":"li","lines":"220,222"}},{"content":"<strong>Describe the Transformer model and its significance in NLP. How does it differ from previous models like RNNs?</strong>","children":[{"content":"The Transformer model relies entirely on self-attention mechanisms to handle sequential data without processing it in order, making it more efficient and capable of capturing long-range dependencies compared to RNNs, which process data sequentially and are less efficient for long sequences.","children":[],"payload":{"tag":"li","lines":"223,224"}}],"payload":{"tag":"li","lines":"222,224"}},{"content":"<strong>What role does the attention mechanism play in modern NLP models, and how does it improve the performance of tasks like translation?</strong>","children":[{"content":"The attention mechanism allows models to focus on specific parts of the input sequence when generating each part of the output, providing contextually relevant information and improving the performance of tasks like translation by focusing on relevant words or phrases.","children":[],"payload":{"tag":"li","lines":"225,226"}}],"payload":{"tag":"li","lines":"224,226"}},{"content":"<strong>What is the difference between fine-tuning and zero-shot learning in the context of large language models?</strong>","children":[{"content":"Fine-tuning involves adapting a pre-trained model to a specific task using additional training data, while zero-shot learning refers to a model's ability to perform tasks it wasn't explicitly trained for by using general knowledge.","children":[],"payload":{"tag":"li","lines":"227,228"}}],"payload":{"tag":"li","lines":"226,228"}},{"content":"<strong>How has the evolution of NLP from rule-based systems to large language models (LLMs) changed the field?</strong>","children":[{"content":"The evolution from rule-based systems to LLMs has transformed NLP by moving from manually coded linguistic rules to data-driven approaches that leverage massive datasets and advanced architectures like Transformers, resulting in models capable of more sophisticated language understanding and generation.","children":[],"payload":{"tag":"li","lines":"229,230"}}],"payload":{"tag":"li","lines":"228,230"}},{"content":"**In the context of NLP, what is a corpus, and how is it used in model training? **","children":[{"content":"A corpus is a large collection of documents that serve as the dataset on which NLP models are trained and evaluated. It is used to analyze linguistic patterns and build statistical models.","children":[],"payload":{"tag":"li","lines":"231,232"}}],"payload":{"tag":"li","lines":"230,232"}}],"payload":{"tag":"h2","lines":"211,212"}}],"payload":{"tag":"h1","lines":"6,7"}},{"maxWidth":300,"initialExpandLevel":1})</script>
</body>
</html>
